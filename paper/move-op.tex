\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[nocompress]{cite} % required by IEEE compsoc
\usepackage{booktabs} % \toprule etc.
\usepackage{amssymb} % \nexists
\usepackage{textcomp} % \textmu
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\usepackage{lineno}
\usepackage{isabelle,isabellesym}
\isabellestyle{it}
\renewenvironment{isabelle}{%
  \medbreak\indent%
  \renewcommand{\isanewline}{\\}%
  \begin{minipage}{\columnwidth}% use minipage to prevent page breaks
  \begin{isabellebody}%
  \begin{tabbing}%
}{%
  \end{tabbing}%
  \end{isabellebody}%
  \end{minipage}%
  \medbreak%
}
\renewcommand{\isacartoucheopen}{}
\renewcommand{\isacartoucheclose}{}

% Don't put a few lines of text on a page that is mostly occupied with figures
\renewcommand{\floatpagefraction}{0.8}

\hyphenation{da-ta-cen-ter da-ta-cen-ters time-stamp time-stamps time-stamped hard-links}

\begin{document}
\title{A highly-available move operation for replicated trees}

% IEEE transactions review is single-blind by default, and double-blind only on special request
% https://www.computer.org/publications/author-resources/peer-review/journals

\author{Martin Kleppmann, Dominic P. Mulligan, Victor B. F. Gomes, and Alastair R. Beresford
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Kleppmann and A.R. Beresford are with the University of Cambridge.\protect\\
\IEEEcompsocthanksitem D.P. Mulligan is with Arm Research, Cambridge, UK.\protect\\
\IEEEcompsocthanksitem V.B.F. Gomes is with Google (this work was done while at the University of Cambridge).}}

\IEEEtitleabstractindextext{%
\begin{abstract}
    Replicated tree data structures are a fundamental building block of distributed filesystems, such as Google Drive and Dropbox, and collaborative applications with a JSON or XML data model.
    These systems need to support a \emph{move} operation that allows a subtree to be moved to a new location within the tree.
    However, such a move operation is difficult to implement correctly if different replicas can concurrently perform arbitrary move operations, and we demonstrate bugs in Google Drive and Dropbox that arise with concurrent moves.
    In this paper we present a CRDT algorithm that handles arbitrary concurrent modifications on trees, while ensuring that the tree structure remains valid (in particular, no cycles are introduced), and guaranteeing that all replicas converge towards the same consistent state.
    Our algorithm requires no synchronous coordination between replicas, making it highly available in the face of network partitions.
    We formally prove the correctness of our algorithm using the Isabelle/HOL proof assistant, and evaluate the performance of our formally verified implementation in a geo-replicated setting.
\end{abstract}

\begin{IEEEkeywords}
Conflict-free Replicated Data Types (CRDTs), formal verification, distributed filesystems, distributed collaboration
\end{IEEEkeywords}}
\maketitle
\IEEEdisplaynontitleabstractindextext

\IEEEraisesectionheading{\section{Introduction}\label{sec:intro}}

\IEEEPARstart{M}{any} applications use a tree-structured data model.
Most filesystems are trees: directories are branch nodes, files are leaf nodes (we discuss hardlinks in \S\ref{sec:extensions}).
XML and JSON documents are also trees, and they are used in many applications to represent e.g.\ rich text (a tree of paragraphs, lists, figures, sections, etc.), vector graphics, CAD drawings, and many other types of data.
Typically, a graphical user interface allows a user to edit this information interactively, resulting in updates to the underlying XML/JSON structure: adding or deleting nodes in the tree, and moving nodes from one position in the tree to another.

In distributed filesystems and collaborative multi-user software, this tree is replicated across multiple nodes.
If users attempt to update the tree concurrently on different replicas, concurrency control is required.
However, standard techniques such as two-phase locking require synchronous coordination between replicas.
If the software is running on mobile devices with unreliable network connectivity, an application based on synchronous coordination becomes unresponsive during network interruptions, leaving users unable to work while they are offline.

If we want to allow offline work, we must allow the system to continue processing read and write requests even in the presence of arbitrary network partitions; in other words, we require high \emph{availability} and \emph{partition-tolerance} in the sense of the CAP theorem~\cite{Gilbert:2002il}.
We can achieve this goal by using \emph{optimistic replication}~\cite{Saito:2005jw}, which means that any replica can make changes to the data without waiting for communication with any other replicas; updates made while disconnected are sent to other replicas later when a network connection is available.
Besides allowing the system to tolerate network partitions, this approach can also improve performance for end users because the response time of a user request is independent of any network latency.

Dropbox and Google Drive are widely-deployed examples of optimistically replicated filesystems: they run a daemon on the user's machine that watches a designated directory for changes.
The user can read and arbitrarily modify the files on their local disk, even while their computer is offline.
However, when the filesystem is concurrently updated on different computers, Google Drive and Dropbox exhibit bugs in their concurrency control, as we show in \S\ref{sec:move-is-hard}.

In this paper we introduce a novel algorithm for handling concurrent updates to a replicated tree, such as an XML document or filesystem.
It allows replicas to manipulate the tree by creating nodes, deleting nodes, or moving subtrees to a new location within the tree.
We rule out bugs like those in Google Drive by formally proving our algorithm correct using the Isabelle/HOL proof assistant.

Our algorithm supports optimistic replication, allowing replicas to temporarily diverge as they are updated, and ensuring that they always converge towards a consistent state.
It is an example of a \emph{Conflict-free Replicated Data Type} or CRDT~\cite{Shapiro:2011un}, and it guarantees a consistency model called \emph{strong eventual consistency}~\cite{Shapiro:2011un,Gomes:2017gy}.
Our contributions are:
\begin{itemize}
    \item We define a Conflict-free Replicated Data Type for trees that allow \emph{move} operations without any coordination between replicas such as locking or consensus.
        As discussed in \S\ref{sec:impossibility}, this has previously been thought to be impossible to achieve \cite{Najafzadeh:2017vk,Najafzadeh:2018bw}.
    \item We formalise the algorithm using Isabelle/HOL \cite{DBLP:conf/tphol/WenzelPN08}, a proof assistant based on higher-order logic, and obtain a computer-checked proof of correctness.
        In particular, we prove that arbitrary concurrent modifications to the tree can be merged such that all replicas converge to a consistent state, while preserving the tree structure.
        Our proof technique can also be applied to other distributed systems, making it of independent interest.
    \item To demonstrate its practical viability, we extract a formally verified Scala implementation of our algorithm from Isabelle.
        We compare its performance to a hand-optimised (not formally verified) implementation and to classic state machine replication.
        In a geo-replicated setup across three continents, state machine replication has approximately four times higher throughput than our algorithm, but our algorithm has up to 100,000 times lower latency.
    \item We perform experiments with Dropbox and Google Drive, and show that they exhibit problems that would be prevented by our algorithm.
\end{itemize}


\begin{figure*}
\centering
\begin{tikzpicture}
  \tikzstyle{time}=[thick,->,gray]
  \tikzstyle{network}=[thick,dashed,blue,-{Stealth[length=3mm]}]
  \node [anchor=east] at (-1.8,3) {Replica 1:};
  \node [anchor=east] at (-1.8,0) {Replica 2:};
  \node [rectangle,draw] (start1) at (0,3) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 2}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}
              child {node {$B$}}
              child {node {$C$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (start2) at (0,0) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 2}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}
              child {node {$B$}}
              child {node {$C$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (change1) at (5,3) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 3}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$B$} child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}}
              child {node {$C$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (change2) at (5,0) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 3}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$B$}}
              child {node {$C$} child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw,inner sep=3mm] (merge1) at (8.2,3) {?};
  \node [rectangle,draw,inner sep=3mm] (merge2) at (8.2,0) {?};
  \draw [time] (start1.east) -- node [above,text width=2.5cm,text centered,inner ysep=5pt,font=\footnotesize] {Move $A$ to be a child of $B$} (change1.west);
  \draw [time] (change1.east) -- (merge1.west);
  \draw [time] (start2.east) -- node [above,text width=2.5cm,text centered,inner ysep=5pt,font=\footnotesize] {Move $A$ to be a child of $C$} (change2.west);
  \draw [time] (change2.east) -- (merge2.west);
  \draw [network] (6.5,0) to [out=90,in=270] (7.2,3);
  \draw [network] (6.5,3) to [out=270,in=90] (7.2,0);
  \node [rotate=90,blue,font=\footnotesize,align=center] at (7.5,1.5) {network\\communication};
  \path [draw,dotted] (-3.1,1.5) -- (8.7,1.5);
  %%%
  \node [fill=red!15] at (9.83,4.10) {(\emph{a})};
  \node [fill=red!15] at (14.18,4.10) {(\emph{b})};
  \node [fill=red!15] at (9.84,1.10) {(\emph{c})};
  \node [fill=red!15] at (14.15,1.10) {(\emph{d})};
  \node [rectangle,draw] at (10.9,3) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{every node}=[text height=5pt,text depth=0pt]
          \tikzstyle{level 1}=[sibling distance=12mm]
          \tikzstyle{level 3}=[sibling distance=4mm]
          \node {$\mathsf{root}$}
              child {node {$B$} child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}}
              child {node {$C$} child {node {$A'$} child {node {$a_1'$}} child {node {$a_2'$}} child {node {$a_3'$}}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] at (13.5,3) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=12mm]
          \node at (0,1.4) {$\mathsf{root}$} child {node (b) {$B$}} child {node (c) {$C$}};
          \tikzstyle{level 1}=[sibling distance=6mm]
          \node (a) at (0,0) {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}};
          \draw (b) -- (a);
          \draw (c) -- (a);
      \end{tikzpicture}
  };
  \node [rectangle,draw] at (10.7,0) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 3}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$B$} child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}}
              child {node {$C$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] at (13.3,0) {
      \begin{tikzpicture}[level distance=7mm]
          \tikzstyle{level 1}=[sibling distance=10mm]
          \tikzstyle{level 3}=[sibling distance=6mm]
          \node {$\mathsf{root}$}
              child {node {$B$}}
              child {node {$C$} child {node {$A$} child {node {$a_1$}} child {node {$a_2$}} child {node {$a_3$}}}};
      \end{tikzpicture}
  };
\end{tikzpicture}
\caption{Replica 1 moves $A$ to be a child of $B$, while concurrently replica 2 moves the same node $A$ to be a child of $C$. Boxes (\emph{a}) to (\emph{d}) show possible outcomes after the replicas have communicated and merged their states.}
\label{fig:move-same-item}
\end{figure*}

\begin{figure*}
\centering
\begin{tikzpicture}
  \tikzstyle{time}=[thick,->,gray]
  \tikzstyle{network}=[thick,dashed,blue,-{Stealth[length=3mm]}]
  \node [anchor=east] at (-1.2,3) {Replica 1:};
  \node [anchor=east] at (-1.2,0) {Replica 2:};
  \node [rectangle,draw] (start1) at (0,3) {
      \begin{tikzpicture}[level distance=7mm]
      \node {$\mathsf{root}$} child {node {$A$} child {node {$C$}}} child {node {$B$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (start2) at (0,0) {
      \begin{tikzpicture}[level distance=7mm]
      \node {$\mathsf{root}$} child {node {$A$} child {node {$C$}}} child {node {$B$}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (change1) at (4.5,3) {
      \begin{tikzpicture}[level distance=7mm]
      \node {$\mathsf{root}$} child {node {$A$} child {node {$B$}} child {node {$C$}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (change2) at (4.5,0) {
      \begin{tikzpicture}[level distance=7mm]
      \node {$\mathsf{root}$} child {node {$B$} child {node {$A$} child {node {$C$}}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw,inner sep=3mm] (merge1) at (8,3) {?};
  \node [rectangle,draw,inner sep=3mm] (merge2) at (8,0) {?};
  \draw [time] (start1.east) -- node [above,text width=2.5cm,text centered,inner ysep=5pt,font=\footnotesize] {Move $B$ to be a child of $A$} (change1.west);
  \draw [time] (change1.east) -- (merge1.west);
  \draw [time] (start2.east) -- node [above,text width=2.5cm,text centered,inner ysep=5pt,font=\footnotesize] {Move $A$ to be a child of $B$} (change2.west);
  \draw [time] (change2.east) -- (merge2.west);
  \draw [network] (6.0,0) to [out=90,in=270] (6.7,3);
  \draw [network] (6.0,3) to [out=270,in=90] (6.7,0);
  \node [rotate=90,blue,font=\footnotesize,align=center] at (7.0,1.5) {network\\communication};
  \path [draw,dotted] (-2.5,1.6) -- (8.5,1.6);
  %%%%%
  \node [rectangle,draw] at (10.95,3) {
      \begin{tikzpicture}[level distance=7mm]
      \useasboundingbox (-1.3,-1.3) rectangle (1,1.2);
      \node [fill=red!15] at (-1.10,1.03) {(\emph{a})};
      \node at (0,1) {$\mathsf{root}$};
      \node (a1) {$A$} child {node (b1) {$B$}} child {node {$C$}};
      \draw (b1.south west) .. controls (-2,-2) and (0,1.5) .. (a1.north);
      \end{tikzpicture}
  };
  \node [rectangle,draw] at (13.8,3) {
      \begin{tikzpicture}[level distance=7mm]
      \useasboundingbox (-1.15,-2.3) rectangle (1.15,0.2);
      \node [fill=red!15] at (0.94,0.03) {(\emph{b})};
      \tikzstyle{level 1}=[sibling distance=12mm]
      \tikzstyle{level 2}=[sibling distance=6mm]
      \node {$\mathsf{root}$}
          child {node {$A$} child {node {$B$}} child {node {$C$}}}
          child {node {$B'$} child {node {$A'$} child {node {$C'$}}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (start) at (10.95,0) {
      \begin{tikzpicture}[level distance=7mm]
      \useasboundingbox (-1.17,-2.3) rectangle (1.15,0.2);
      \node [fill=red!15] at (-0.98,0.03) {(\emph{c})};
      \node {$\mathsf{root}$} child {node {$A$} child {node {$B$}} child {node {$C$}}};
      \end{tikzpicture}
  };
  \node [rectangle,draw] (right) at (13.8,0) {
      \begin{tikzpicture}[level distance=7mm]
      \useasboundingbox (-1.15,-2.3) rectangle (1.15,0.2);
      \node [fill=red!15] at (0.93,0.03) {(\emph{d})};
      \node {$\mathsf{root}$} child {node {$B$} child {node {$A$} child {node {$C$}}}};
      \end{tikzpicture}
  };
\end{tikzpicture}
\caption{Initially, nodes $A$ and $B$ are siblings. Replica 1 moves $B$ to be a child of $A$, while concurrently replica 2 moves $A$ to be a child of $B$. Boxes (\emph{a}) to (\emph{d}) show possible outcomes after the replicas have communicated and merged their states.}\label{fig:move-cycle}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth,keepaspectratio=true]{gdrive-error.png}
  \caption{Error message produced by Google Drive Backup and Sync on Mac OS as a result of performing the operations shown in Figure~\ref{fig:move-cycle}, indicating a bug in the underlying replication algorithm.}
  \label{fig:gdrive-error}
\end{figure*}

\section{Why a move operation is hard}\label{sec:move-is-hard}

Applications that rely on a tree data model often need to move a node from one location to another location within the tree, such that all of its children move along with it:
\begin{itemize}
    \item In a filesystem, any file or directory can be moved to a different parent directory.
        Moreover, renaming a file or directory is equivalent to moving it to a new name without changing its parent directory.
    \item In a rich text editor, a paragraph can be turned into a bullet point.
        In the XML tree this corresponds to creating new list and bullet point nodes, and then moving the paragraph node inside the bullet point.
    \item In graphics software, grouping two objects corresponds to creating a new group node, and then moving the two objects into the new group node.
\end{itemize}
As these operations are so common, it is not obvious why a move operation should be difficult in a replicated setting.
In this section we demonstrate some problems that arise with replicated trees, before proceeding to our solution in \S\ref{sec:algorithm}.

\subsection{Concurrent moves of the same node}\label{sec:move-same-item}

The first difficulty arises when the same node is concurrently moved into different locations on different replicas.
This scenario is illustrated in Figure~\ref{fig:move-same-item}, where replica 1 moves node $A$ to be a child of $B$, while concurrently replica 2 moves $A$ to be a child of $C$.
After the replicas communicate, what should the merged state of the tree be?

If a move operation is implemented by deleting the moved subtree from its old location, and then re-creating it at the new location, the merged state will be as shown in Figure \ref{fig:move-same-item}a: the concurrent moves will duplicate the moved subtree, since each move independently recreates the subtree in each destination location.
We believe that this duplication is undesirable, since subsequent edits to nodes in the duplicated subtree will apply to only one of the copies.
Two users who believe they are collaborating on the same file may in fact be editing two different copies, which will then become inconsistent with each other.
In the rich text editor and graphics software examples, such duplication is also undesirable.

Another possible resolution is for the destination locations of both moves to refer to the same node, as shown in Figure~\ref{fig:move-same-item}b.
However, the result is a DAG, not a tree.
POSIX filesystems do not allow this outcome, since they do not allow hardlinks to directories.

In our opinion, the only reasonable outcomes are those shown in Figure~\ref{fig:move-same-item}c and~\ref{fig:move-same-item}d: the moved subtree appears either in replica 1's destination location or in replica 2's destination location, but not in both.
Which one of these two is picked is arbitrary, due to the symmetry between the two replicas.
The ``winning'' location could be picked based on a timestamp in the operations, similarly to the ``last writer wins'' conflict resolution method of Thomas's write rule~\cite{Johnson:1975we}.
The \emph{timestamp} in this context need not come from a physical clock; it could also be logical, such as a Lamport timestamp~\cite{Lamport:1978jq}.

We tested this scenario with file sync products Dropbox and Google Drive by concurrently moving the same directory to two different destination directories.\footnote{Experiment setup: we installed the official Mac OS clients for Dropbox and Google Drive on two computers, logged into the same Dropbox/Google accounts, and configured them to sync a directory on the local filesystem.
To test concurrent operations, we disconnected both computers from the Internet, performed a move operation on the local filesystem of each computer, then reconnected and waited for them to sync.}
Dropbox exhibited the undesirable duplication behaviour of Figure~\ref{fig:move-same-item}a, while the outcome on Google Drive was as in Figure~\ref{fig:move-same-item}c/d.

\subsection{Moving a node to be a descendant of itself}\label{sec:move-cycle}

On a filesystem, the destination directory of a move operation must not be a subdirectory of the directory being moved.
For example, if \texttt{b} is a subdirectory of \texttt{a}, then the Unix shell command \texttt{mv a a/b/} will fail with an error.
This restriction is required because allowing this operation would introduce a cycle into the directory graph, and so the filesystem would no longer be a tree.
Any tree data structure that supports a move operation must handle this case.

In an unreplicated tree it is easy to prevent cycles being introduced: if the node being moved is an ancestor of the destination node, the operation is rejected.
However, in a replicated setting, different replicas may perform operations that are individually safe, but whose combination leads to a cycle.
One such example is illustrated in Figure~\ref{fig:move-cycle}.
Here, replica 1 moves $B$ to be a child of $A$, while concurrently replica 2 moves $A$ to be a child of $B$.
As each replica propagates its operation to the other replica, a careless implementation might end up in the state shown in Figure~\ref{fig:move-cycle}a: $A$ and $B$ have formed a cycle, detached from the tree.

Another possible outcome is shown in Figure~\ref{fig:move-cycle}b: the nodes involved in the concurrent moves (and their children) could be duplicated, so that both ``$A$ as a child of $B$'' and ``$B$ as a child of $A$'' can exist in the tree.
However, such duplication is undesirable for the same reasons as in \S\ref{sec:move-same-item}.

In our opinion, the best way of handling the conflicting operations of Figure~\ref{fig:move-cycle} is to choose either Figure~\ref{fig:move-cycle}c or~\ref{fig:move-cycle}d: that is, either the result of applying replica 1's operation and ignoring replica 2's operation, or vice versa.
Like in \S\ref{sec:move-same-item}, the winning operation can be picked based on a timestamp.

As before, we tested this scenario with Google Drive and Dropbox.
In Google Drive, one replica was able to successfully sync with the server, while the other replica displayed the ``unknown error'' message shown in Figure~\ref{fig:gdrive-error}.
The replica in an error state refused to sync the conflicting directory, and its filesystem state remained permanently inconsistent with the other replica.
This error state persisted until the directories on the erroring replica were manually moved to match the state of the other replica.
We have reported this bug to Google.
On the other hand, Dropbox exhibited the duplication behaviour shown in Figure~\ref{fig:move-cycle}b.

\subsection{Is a highly-available move operation impossible?}\label{sec:impossibility}

Najafzadeh et al.~\cite{Najafzadeh:2017vk,Najafzadeh:2018bw} previously implemented a replicated filesystem with a move operation, and analysed the case of concurrent move operations introducing a cycle.
Using the CISE proof tool~\cite{DBLP:conf/popl/GotsmanYFNS16,Najafzadeh:2016fi} the authors confirm that it is not sufficient for the replica that generates a move operation to check whether the operation introduces a cycle: like in Figure~\ref{fig:move-cycle}, two concurrent operations may be safe individually, but introduce a cycle when combined.

Najafzadeh et al. propose two solutions to this problem: either to duplicate tree nodes, as in Figure~\ref{fig:move-cycle}b, or to execute a synchronous locking protocol that prevents two move operations from concurrently modifying the same part of the tree.
The downside of a locking protocol is that the move operation is no longer highly available in the presence of network partitions, since it must wait for synchronous communication with other replicas or a lock server.

While these solutions are valid, the authors go on to claim that ``no file system can support an unsynchronised move without anomalies, such as loss or duplication''~\cite{Najafzadeh:2018bw}.
We refute that claim in this paper: our algorithm does not perform any locking, coordination or synchronisation among replicas, but it nevertheless ensures that the tree invariants are always satisfied (in particular, it never introduces cycles), and it never duplicates or loses any tree nodes.
To our knowledge, our algorithm is the first to provide all of these properties simultaneously.
We give a precise specification of our algorithm's consistency properties in \S\ref{sec:proof}.


\begin{figure*}
\raggedright
\begin{isabellebody}
\internallinenumbers
\isacommand{datatype}\isamarkupfalse%
\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ operation\isanewline
\ \ {\isacharequal}\ {\rm Move}\ {\isacharparenleft}move{\isacharunderscore}time{\isacharcolon}\ {\isacharprime}t{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ {\isacharparenleft}move{\isacharunderscore}parent{\isacharcolon}\ {\isacharprime}n{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ {\isacharparenleft}move{\isacharunderscore}meta{\isacharcolon}\ {\isacharprime}m{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ {\isacharparenleft}move{\isacharunderscore}child{\isacharcolon}\ {\isacharprime}n{\isacharparenright}\isanewline
\isanewline
\isacommand{datatype}\isamarkupfalse%
\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ log{\isacharunderscore}op\isanewline
\ \ {\isacharequal}\ {\rm LogMove}\ {\isacharparenleft}log{\isacharunderscore}time{\isacharcolon}\ {\isacharprime}t{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ \ \ \ {\isacharparenleft}old{\isacharunderscore}parent{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m{\isacharparenright}\ option{\isacartoucheclose}{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ \ \ \ {\isacharparenleft}new{\isacharunderscore}parent{\isacharcolon}\ {\isacharprime}n{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ \ \ \ {\isacharparenleft}log{\isacharunderscore}meta{\isacharcolon}\ {\isacharprime}m{\isacharparenright}\isanewline
\ \ \ \ \ \ \ \ \ \ \ \ {\isacharparenleft}log{\isacharunderscore}child{\isacharcolon}\ {\isacharprime}n{\isacharparenright}\isanewline
\isanewline
\isacommand{type{\isacharunderscore}synonym}\isamarkupfalse%
\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state\ {\isacharequal}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ log{\isacharunderscore}op\ list\ {\isasymtimes}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set{\isacartoucheclose}\isanewline
\isanewline
\isacommand{definition}\isamarkupfalse%
\ get{\isacharunderscore}parent\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ {\isacharprime}n\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m{\isacharparenright}\ option{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}get{\isacharunderscore}parent\ tree\ child\ {\isasymequiv}\isanewline
\ \ \ \ \ \isacommand{if}\ {\isasymexists}{\isacharbang}parent{\isachardot}\ {\isasymexists}{\isacharbang}meta{\isachardot}\ {\isacharparenleft}parent{\isacharcomma}\ meta{\isacharcomma}\ child{\isacharparenright}\ {\isasymin}\ tree\ \isacommand{then}\isanewline
\ \ \ \ \ \ \ {\rm Some}\ {\isacharparenleft}{\rm THE}\ {\isacharparenleft}parent{\isacharcomma}\ meta{\isacharparenright}{\isachardot}\ {\isacharparenleft}parent{\isacharcomma}\ meta{\isacharcomma}\ child{\isacharparenright}\ {\isasymin}\ tree{\isacharparenright}\isanewline
\ \ \ \ \ \isacommand{else}\ {\rm None}{\isacartoucheclose}\isanewline
\isanewline
\isacommand{inductive}\isamarkupfalse%
\ ancestor\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ {\isacharprime}n\ {\isasymRightarrow}\ {\isacharprime}n\ {\isasymRightarrow}\ bool{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}{\isasymlbrakk}{\isacharparenleft}parent{\isacharcomma}\ meta{\isacharcomma}\ child{\isacharparenright}\ {\isasymin}\ tree{\isasymrbrakk}\ {\isasymLongrightarrow}\ ancestor\ tree\ parent\ child{\isacartoucheclose}\ {\isacharbar}\isanewline
\ \ {\isacartoucheopen}{\isasymlbrakk}{\isacharparenleft}parent{\isacharcomma}\ meta{\isacharcomma}\ child{\isacharparenright}\ {\isasymin}\ tree{\isacharsemicolon}\ ancestor\ tree\ anc\ parent{\isasymrbrakk}\ {\isasymLongrightarrow}\ ancestor\ tree\ anc\ child{\isacartoucheclose}\isanewline
\isanewline
\isacommand{fun}\isamarkupfalse%
\ do{\isacharunderscore}op\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ operation\ {\isasymtimes}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ log{\isacharunderscore}op\ {\isasymtimes}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}do{\isacharunderscore}op\ {\isacharparenleft}{\rm Move}\ t\ newp\ m\ c{\isacharcomma}\ tree{\isacharparenright}\ {\isacharequal}\isanewline
\ \ \ \ \ {\isacharparenleft}{\rm LogMove}\ t\ {\isacharparenleft}get{\isacharunderscore}parent\ tree\ c{\isacharparenright}\ newp\ m\ c{\isacharcomma}\isanewline
\ \ \ \ \ \ \isacommand{if}\ ancestor\ tree\ c\ newp\ {\isasymor}\ c\ {\isacharequal}\ newp\ \isacommand{then}\ tree\isanewline
\ \ \ \ \ \ \isacommand{else}\ {\isacharbraceleft}{\isacharparenleft}p{\isacharprime}{\isacharcomma}\ m{\isacharprime}{\isacharcomma}\ c{\isacharprime}{\isacharparenright}\ {\isasymin}\ tree{\isachardot}\ c{\isacharprime}\ {\isasymnoteq}\ c{\isacharbraceright}\ {\isasymunion}\ {\isacharbraceleft}{\isacharparenleft}newp{\isacharcomma}\ m{\isacharcomma}\ c{\isacharparenright}{\isacharbraceright}{\isacharparenright}{\isacartoucheclose}\isanewline
\isanewline
\isacommand{fun}\isamarkupfalse%
\ undo{\isacharunderscore}op\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ log{\isacharunderscore}op\ {\isasymtimes}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set{\isacartoucheclose}\ \isakeyword{where}\isanewline
    \ \ {\isacartoucheopen}undo{\isacharunderscore}op\ {\isacharparenleft}{\rm LogMove}\ t\ {\rm None}\ newp\ m\ c{\isacharcomma}\ tree{\isacharparenright}\ {\isacharequal}\ {\isacharbraceleft}{\isacharparenleft}p{\isacharprime}{\isacharcomma}\ m{\isacharprime}{\isacharcomma}\ c{\isacharprime}{\isacharparenright}\ {\isasymin}\ tree{\isachardot}\ c{\isacharprime}\ {\isasymnoteq}\ c{\isacharbraceright}{\isacartoucheclose}\ {\isacharbar}\isanewline
\ \ {\isacartoucheopen}undo{\isacharunderscore}op\ {\isacharparenleft}{\rm LogMove}\ t\ {\isacharparenleft}{\rm Some}\ {\isacharparenleft}oldp{\isacharcomma}\ oldm{\isacharparenright}{\isacharparenright}\ newp\ m\ c{\isacharcomma}\ tree{\isacharparenright}\ {\isacharequal}\isanewline
\ \ \ \ \ {\isacharbraceleft}{\isacharparenleft}p{\isacharprime}{\isacharcomma}\ m{\isacharprime}{\isacharcomma}\ c{\isacharprime}{\isacharparenright}\ {\isasymin}\ tree{\isachardot}\ c{\isacharprime}\ {\isasymnoteq}\ c{\isacharbraceright}\ {\isasymunion}\ {\isacharbraceleft}{\isacharparenleft}oldp{\isacharcomma}\ oldm{\isacharcomma}\ c{\isacharparenright}{\isacharbraceright}{\isacartoucheclose}\isanewline
\isanewline
\isacommand{fun}\isamarkupfalse%
\ redo{\isacharunderscore}op\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ log{\isacharunderscore}op\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}redo{\isacharunderscore}op\ {\isacharparenleft}{\rm LogMove}\ t\ {\isacharunderscore}\ p\ m\ c{\isacharparenright}\ {\isacharparenleft}ops{\isacharcomma}\ tree{\isacharparenright}\ {\isacharequal}\isanewline
\ \ \ \ \ {\isacharparenleft}\isacommand{let}\ {\isacharparenleft}op{\isadigit{2}}{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}\ {\isacharequal}\ do{\isacharunderscore}op\ {\isacharparenleft}{\rm Move}\ t\ p\ m\ c{\isacharcomma}\ tree{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{in}\ {\isacharparenleft}op{\isadigit{2}}\ {\isacharhash}\ ops{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isacartoucheclose}\isanewline
\isanewline
\isacommand{fun}\isamarkupfalse%
\ apply{\isacharunderscore}op\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcolon}{\isacharcolon}{\isacharbraceleft}linorder{\isacharbraceright}{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ operation\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}apply{\isacharunderscore}op\ op{\isadigit{1}}\ {\isacharparenleft}{\isacharbrackleft}{\isacharbrackright}{\isacharcomma}\ tree{\isadigit{1}}{\isacharparenright}\ {\isacharequal}\isanewline
\ \ \ \ \ {\isacharparenleft}\isacommand{let}\ {\isacharparenleft}op{\isadigit{2}}{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}\ {\isacharequal}\ do{\isacharunderscore}op\ {\isacharparenleft}op{\isadigit{1}}{\isacharcomma}\ tree{\isadigit{1}}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{in}\ {\isacharparenleft}{\isacharbrackleft}op{\isadigit{2}}{\isacharbrackright}{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isacartoucheclose}\ {\isacharbar}\isanewline
\ \ {\isacartoucheopen}apply{\isacharunderscore}op\ op{\isadigit{1}}\ {\isacharparenleft}logop\ {\isacharhash}\ ops{\isacharcomma}\ tree{\isadigit{1}}{\isacharparenright}\ {\isacharequal}\isanewline
\ \ \ \ \ {\isacharparenleft}\isacommand{if}\ move{\isacharunderscore}time\ op{\isadigit{1}}\ {\isacharless}\ log{\isacharunderscore}time\ logop\isanewline
\ \ \ \ \ \ \isacommand{then}\ redo{\isacharunderscore}op\ logop\ {\isacharparenleft}apply{\isacharunderscore}op\ op{\isadigit{1}}\ {\isacharparenleft}ops{\isacharcomma}\ undo{\isacharunderscore}op\ {\isacharparenleft}logop{\isacharcomma}\ tree{\isadigit{1}}{\isacharparenright}{\isacharparenright}{\isacharparenright}\isanewline
\ \ \ \ \ \ \isacommand{else}\ \isacommand{let}\ {\isacharparenleft}op{\isadigit{2}}{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}\ {\isacharequal}\ do{\isacharunderscore}op\ {\isacharparenleft}op{\isadigit{1}}{\isacharcomma}\ tree{\isadigit{1}}{\isacharparenright}\ \isacommand{in}\ {\isacharparenleft}op{\isadigit{2}}\ {\isacharhash}\ logop\ {\isacharhash}\ ops{\isacharcomma}\ tree{\isadigit{2}}{\isacharparenright}{\isacharparenright}{\isacartoucheclose}\isanewline
\isanewline
\isacommand{definition}\isamarkupfalse%
\ apply{\isacharunderscore}ops\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}t{\isacharcolon}{\isacharcolon}{\isacharbraceleft}linorder{\isacharbraceright}{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ operation\ list\ {\isasymRightarrow}\ {\isacharparenleft}{\isacharprime}t{\isacharcomma}\ {\isacharprime}n{\isacharcomma}\ {\isacharprime}m{\isacharparenright}\ state{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}apply{\isacharunderscore}ops\ ops\ {\isasymequiv}\ foldl\ {\isacharparenleft}{\isasymlambda}state\ oper{\isachardot}\ apply{\isacharunderscore}op\ oper\ state{\isacharparenright}\ {\isacharparenleft}{\isacharbrackleft}{\isacharbrackright}{\isacharcomma}\ {\isacharbraceleft}{\isacharbraceright}{\isacharparenright}\ ops{\isacartoucheclose}\isanewline
\isanewline
\isacommand{definition}\isamarkupfalse%
\ unique{\isacharunderscore}parent\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ bool{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}unique{\isacharunderscore}parent\ tree\ {\isasymequiv}\isanewline
\ \ \ \ {\isacharparenleft}{\isasymforall}p{\isadigit{1}}\ p{\isadigit{2}}\ m{\isadigit{1}}\ m{\isadigit{2}}\ c{\isachardot}\ {\isacharparenleft}p{\isadigit{1}}{\isacharcomma}\ m{\isadigit{1}}{\isacharcomma}\ c{\isacharparenright}\ {\isasymin}\ tree\ {\isasymand}\ {\isacharparenleft}p{\isadigit{2}}{\isacharcomma}\ m{\isadigit{2}}{\isacharcomma}\ c{\isacharparenright}\ {\isasymin}\ tree\ {\isasymlongrightarrow}\ p{\isadigit{1}}\ {\isacharequal}\ p{\isadigit{2}}\ {\isasymand}\ m{\isadigit{1}}\ {\isacharequal}\ m{\isadigit{2}}{\isacharparenright}{\isacartoucheclose}\isanewline%
\isanewline
\isacommand{definition}\isamarkupfalse%
\ acyclic\ {\isacharcolon}{\isacharcolon}\ {\isacartoucheopen}{\isacharparenleft}{\isacharprime}n\ {\isasymtimes}\ {\isacharprime}m\ {\isasymtimes}\ {\isacharprime}n{\isacharparenright}\ set\ {\isasymRightarrow}\ bool{\isacartoucheclose}\ \isakeyword{where}\isanewline
\ \ {\isacartoucheopen}acyclic\ tree\ {\isasymequiv}\ {\isacharparenleft}{\isasymnexists}n{\isachardot}\ ancestor\ tree\ n\ n{\isacharparenright}{\isacartoucheclose}
\end{isabellebody}
\caption{The move operation algorithm, implemented in the Isabelle/HOL language.}
\label{fig:code}
\end{figure*}

\section{The replicated tree algorithm}\label{sec:algorithm}

We now introduce our algorithm for a replicated tree that supports a move operation.
We model each replica as a state machine that transitions from one state to the next by applying an operation.
The algorithm is executed independently on each replica with no shared memory between replicas.

When the user wants to make a change to the tree, they generate an operation and apply it to their local replica.
Every operation is also asynchronously sent over the network to all other replicas, and applied by every remote replica using the same algorithm as for local operations.
A replica may communicate directly with any other replica.
The network may arbitrarily delay or reorder messages, but we assume that an underlying network protocol detects and retransmits lost messages, and suppresses duplicates.
We do not assume any central server or consensus protocol.
Any number of replicas may fail by crashing, and any non-crashed subset of replicas can continue executing operations.

The key consistency property of our algorithm is \emph{convergence}: that is, whenever any two replicas have applied the same set of operations, then they must be in the same state---even if the operations were applied in a different order on different replicas.
We prove this in \S\ref{sec:proof} by showing that applying operations using our algorithm is commutative.

Figure~\ref{fig:code} gives the full source code for our algorithm in the Isabelle/HOL language~\cite{DBLP:conf/tphol/WenzelPN08}.
We choose this language because it combines the conciseness of pseudocode with the precision of mathematical notation.
It supports formal reasoning, allowing us to prove the correctness of the algorithm (\S\ref{sec:proof}), and it can be exported to Scala, Haskell, or OCaml.

In this section we walk through the code step by step, explaining the Isabelle/HOL syntax as we encounter it.
Additional background documentation is available~\cite{DBLP:books/sp/NipkowK14}.

\subsection{Operations and trees}\label{sec:ops-trees}

We allow the tree to be updated in three ways: by creating a new child of any parent node, by deleting a node, or by moving a node to be a child of a new parent.
However, all three types of update can be represented by a move operation.
To create a node, we generate a fresh ID for that node, and issue an operation to move this new ID to be a child of its desired parent; the node is then implicitly created.
We also designate as ``trash'' some node ID that does not exist in the tree; then we can delete a node by moving it to be a child of the trash.
Node creation and deletion are discussed further in \S\ref{sec:create}.

Thus, we define one kind of operation: \isa{Move t p m c} (Figure \ref{fig:code}, lines 1--5).
A move operation is a 4-tuple consisting of a timestamp \isa{t} of type \isa{'t}, a parent node ID \isa{p} of type \isa{'n}, a metadata field \isa{m} of type \isa{'m}, and a child node ID \isa{c} of type \isa{'n}.
Here, \isa{'t}, \isa{'n} and \isa{'m} are \emph{type variables} that can be replaced with arbitrary types; we only require that node identifiers \isa{'n} are globally unique (e.g.\ UUIDs); timestamps \isa{'t} need to be globally unique and totally ordered (e.g.\ Lamport timestamps~\cite{Lamport:1978jq}).

The meaning of an operation \isa{Move t p m c} is that at time \isa{t}, the node with ID \isa{c} is moved to be a child of the parent node with ID \isa{p}.
The operation does not specify the old location of \isa{c}; the algorithm simply removes \isa{c} from wherever it is currently located in the tree, and moves it to \isa{p}.
If \isa{c} does not currently exist in the tree, it is created as a child of \isa{p}.

The metadata field \isa{m} in a move operation allows additional information to be associated with the parent-child relationship of \isa{p} and \isa{c}.
For example, in a filesystem, the parent and child are the inodes of a directory and a file within it, respectively, and the metadata contains the filename of the child.
Thus, a file with inode \isa{c} can be renamed by performing a \isa{Move t p m c}, where the new parent directory \isa{p} is the inode of the existing parent (unchanged), but the metadata \isa{m} contains the new filename.

When users want to make changes to the tree on their local replica, they generate new \isa{Move t p m c} operations for these changes, and apply these operations using the algorithm described in the rest of this section.

We can represent the tree as a set of \isa{(parent, meta, child)} triples, denoted in Isabelle/HOL as \isa{('n {\isasymtimes} 'm {\isasymtimes} 'n) set}.
When we have \isa{(p, m, c) {\isasymin} tree}, that means \isa{c} is a child of \isa{p} in the tree, with associated metadata \isa{m}.
Given a \isa{tree}, we can construct a new \isa{tree'} in which the child \isa{c} is moved to a new parent \isa{p}, with associated metadata \isa{m}, as follows:
\begin{isabelle}
tree' = \{(p', m', c') {\isasymin} tree. c' {\isasymnoteq} c\} {\isasymunion} \{(p, m, c)\}
\end{isabelle}
That is, we remove any existing parent-child relationship for \isa{c} from the set \isa{tree}, and then add \isa{\{(p, m, c)\}} to represent the new parent-child relationship.
This expression appears on lines 30 and 35 of Figure~\ref{fig:code}, as we shall explain shortly.

\subsection{Replica state and operation log}\label{sec:state-log}

In order to correctly apply move operations, a replica needs to maintain not only the current state of the tree, but also an \emph{operation log}.
The log is a list of \isa{LogMove} records in descending timestamp order.
\isa{LogMove t oldp p m c} (lines 7--12) is similar to \isa{Move t p m c}; the difference is that \isa{LogMove} has an additional field \isa{oldp} of type \isa{('n {\isasymtimes} 'm) option}.
This \isa{option} type means the field can either take the value \isa{None} (similar to null), or a pair of a node ID and a metadata field.

When a replica applies a \isa{Move} operation to its tree, it also records a corresponding \isa{LogMove} operation in its log.
The \isa{t}, \isa{p}, \isa{m} and \isa{c} fields are taken directly from the \isa{Move} record, while the \isa{oldp} field is filled in based on the state of the tree before the move.
If \isa{c} did not exist in the tree, \isa{oldp} is set to \isa{None}.
Otherwise, \isa{oldp} records the previous parent and metadata of \isa{c}: if there exist \isa{p'} and \isa{m'} such that \isa{(p', m', c) {\isasymin} tree}, then \isa{oldp} is set to \isa{Some (p', m')}.

The \isa{get\_parent} function (lines 16--20) implements this.
In the first line of \isa{get\_parent}, the expression between \isa{::} and \textbf{where} is the type signature of the function, in this case:
\begin{quote}
\isa{('n {\isasymtimes} 'm {\isasymtimes} 'n) set {\isasymRightarrow} 'n {\isasymRightarrow} ('n {\isasymtimes} 'm) option}
\end{quote}
This signature denotes a function that takes two arguments: a tree \isa{('n {\isasymtimes} 'm {\isasymtimes} 'n) set} and a node ID \isa{'n}.
It then returns a \isa{('n {\isasymtimes} 'm) option}.
The operator \isa{{\isasymexists}!x} means ``there exists a unique value \isa{x} such that\dots'', while \isa{THE x} means ``\emph{choose} the unique value \isa{x} such that\ldots''.

In line 14 we define the datatype for the state of a replica: a pair \isa{(log, tree)} where \isa{log} is a list of \isa{LogMove} records, and \isa{tree} is a set of \isa{(parent, meta, child)} triples as before.

\subsection{Preventing cycles}\label{sec:prevent-cycles}

Recall from \S\ref{sec:move-cycle} that in order to prevent a cycle being introduced, the node being moved must not be an ancestor of the destination node.
To implement this we first define the \isa{ancestor} relation in lines 22--24.
It is the transitive closure of a tree's parent-child relation: if \isa{(p, m, c) {\isasymin} tree} then \isa{p} is an ancestor of \isa{c} (line 23); moreover, if \isa{a} is an ancestor of \isa{p} and \isa{(p, m, c) {\isasymin} tree}, then \isa{a} is also an ancestor of \isa{c} (line 24).
The \textbf{inductive} keyword indicates that this recursive definition is iterated until the least fixed point is reached.

The \isa{do\_op} function (lines 26--30) now performs the actual work of applying a move operation.
This function takes as argument a pair consisting of a \isa{Move} operation and the current tree, and it returns a pair consisting of a \isa{LogMove} operation (which will be added to the log) and an updated tree.
In line 28, the \isa{LogMove} record is constructed as described in \S\ref{sec:state-log}, obtaining the prior parent and metadata of \isa{c} using the \isa{get\_parent} function.

Line 29 performs the check that ensures no cycles are introduced: if \isa{ancestor tree c newp}, i.e.\ if the node \isa{c} is being moved, and \isa{c} is an ancestor of the new parent \isa{newp}, then the tree is returned unmodified---in other words, the operation is ignored.
Similarly, the operation is also ignored if \isa{c = newp}.
Otherwise (line 30), the tree is updated by removing \isa{c} from its existing parent, if any, and adding the new parent-child relationship \isa{(newp, m, c)} to the tree.

\subsection{Applying operations in any order}\label{sec:applying}

The \isa{do\_op} function is sufficient for applying operations if all replicas apply operations in the same order.
However, in an optimistic replication setting, each replica may apply the operations in a different order, and we need to ensure that the replica state nevertheless converges towards a consistent state.
This goal is accomplished by the \isa{undo\_op}, \isa{redo\_op}, and \isa{apply\_op} functions (lines 32--49).

When a replica needs to apply an operation with timestamp \isa{t}, it first undoes the effect of any operations with a timestamp greater than \isa{t}, then performs the new operation, and finally re-applies the undone operations.
As a result, the state of the tree is as if the operations had all been applied in order of increasing timestamp, even though in fact they might have been applied in any order.

The \isa{apply\_op} function (lines 42--49) takes two arguments: a \isa{Move} operation to apply and the current replica state; and it returns the new replica state.
The constraint \isa{'t::\{linorder\}} in the type signature indicates that timestamps \isa{'t} are instances of the \isa{linorder} type class, and they can therefore be compared with the $<$ operator defining a linear (or total) order.
This comparison occurs on line 47.

Recall that the replica state includes the operation log (line 14), and we use this log to perform the undo-do-redo cycle.
Lines 43--45 handle the case where the log is empty: in this case, we simply perform the operation using \isa{do\_op}, and return the new tree along with a log containing a single \isa{LogMove} record.
If the log is nonempty (line 46), we take \isa{logop} to be the first element of the log, and \isa{ops} to be the rest.
(The hash character in \isa{logop {\isacharhash} ops} is the \emph{list cons} operator that adds one element to the head of a list.)
If the timestamp of \isa{logop} is greater than the timestamp of the new operation (line 47) we first undo \isa{logop} with \isa{undo\_op}, then recursively apply the new operation to the remaining log, and finally reapply \isa{logop} with \isa{redo\_op} (line 48).
Otherwise we perform the operation using \isa{do\_op}, and add the corresponding \isa{LogMove} record as the head of the log (line 49).

This logic ensures that the log is maintained in descending timestamp order, with the greatest timestamp at the head.
\isa{undo\_op} (lines 32--35) inverts the effect of a previous move operation by restoring the prior parent and metadata that were recorded in the \isa{LogMove}'s additional field.
\isa{redo\_op} (lines 37--40) uses \isa{do\_op} to perform an operation again and recomputes the \isa{LogMove} record (which might have changed due to the effect of the new operation).

\subsection{Handling conflicts}\label{sec:conflicts}

Due to the undo-do-redo cycle, the state of the tree is as if all operations had been applied using \isa{do\_op} in increasing timestamp order, regardless of the order in which they were actually applied.
This provides a clear and consistent approach to the handling of conflicts:
\begin{itemize}
    \item If two operations concurrently move the same node, the operation with the lower timestamp moves the node first, and then the operation with the greater timestamp moves it again, so the final parent is determined by the latter.
        Since move operations do not specify the old location of a node, but only the new location, this sequential execution of concurrent operations is well-defined.
        In Figure~\ref{fig:move-same-item}, the outcome is (c) if the operation from replica 1 has the greater timestamp, or (d) if the operation from replica 2 has the greater timestamp.
    \item If two operations would introduce a cycle when combined, as in \S\ref{sec:move-cycle}, then the operation with the greater timestamp is ignored because \isa{do\_op} checks for cycles based on the tree created by all operations with a lower timestamp.
        The lower of two conflicting operations will take effect, since that operation by itself is safe.
        When the higher-timestamped operation is applied, \isa{do\_op} detects that it would introduce a cycle, and therefore ignores the operation.
        In the example of Figure~\ref{fig:move-cycle}, the outcome is (c) if the operation from replica 1 has the lower timestamp, or (d) if the operation from replica 2 has the lower timestamp.
\end{itemize}

This resolution of a conflict between two operations can be generalised to any number of conflicting operations by repeatedly applying the rules pairwise.

Note that the safety of an operation (whether or not it would introduce a cycle) may change as subsequent operations with lower timestamps are applied.
For example, an operation may initially be regarded as safe, and then be reclassified as unsafe after applying a conflicting operation with a lower timestamp.
The opposite is also possible: an operation previously regarded as unsafe may become safe through the application of an operation that removes the risk of introducing a cycle.
For this reason, the operation log must include all operations, even those that were ignored.

One final type of conflict that we have not discussed so far is multiple child nodes with the same parent and the same metadata.
For example, in a filesystem, two users could concurrently create files with the same name in the same directory.
Our algorithm does not prevent such a conflict, but simply retains both child nodes.
In practice, the collision would be resolved by making the filenames distinct, e.g.\ by appending a replica identifier to the filenames.

\subsection{Node creation and deletion}\label{sec:create}

As discussed previously, no separate operations for node creation and deletion are needed, since a node is implicitly created when it is first moved, and a node can be deleted by moving it to a designated trash node outside of the tree.
By avoiding a distinction between different operation types we simplify the algorithm and proofs of correctness, but we potentially sacrifice performance in applications that mostly create and delete nodes, and only occasionally move nodes.
Ideally, we would want to incur the costs of the undo-do-redo process only for genuine move operations, and not for node creation and deletion.

This optimisation is possible for operations that create new tree nodes: such operations can be directly applied to the tree without any undo/redo by using \isa{do\_op} instead of \isa{apply\_op}, and they do not need to recorded in the log.
We have proved in Isabelle that this optimisation is safe, under the following additional assumptions: 1.~the parent node in any move/create operation must exist in the tree; 2.~the creation operation for a given node must be unique (i.e.\ there cannot be two operations that both create the same node); 3.~a node creation operation is applied before any operation that moves the created node.
These assumptions are easily satisfied in practice.

For node deletion we have not been able to find a similar optimisation.
Deletion operations must go through the undo-do-redo process because deleting a node may cause a previously unsafe move operation with a greater timestamp to become safe by breaking a particular ancestor relationship; and as the previously ignored operation takes effect, we must re-evaluate all operations with greater timestamps to determine whether they are safe or not.

When a node is deleted, any children of the deleted node remain in the tree, but since they are no longer reachable from the root, they effectively become invisible to the application.
We do not recursively remove children, since a move operation concurrent to the deletion operation might move the deleted subtree back out of the trash and into the visible tree; in this scenario we want all children of the moved subtree to be preserved unmodified.

\subsection{Algorithm extensions}\label{sec:extensions}

\noindent\textbf{Hardlinks and symlinks.}
Unix filesystems support hardlinks, which allow the same file inode to be referenced from multiple locations in a tree.
Our tree data structure can easily be extended to support this: rather than placing file data directly in the leaf nodes of the tree, the leaf node must reference the file inode.
Thus, references to the same file inode can appear in multiple leaf nodes of the tree.
Symlinks are also easy to support, since they are just leaf nodes containing a path (not a reference to an inode).

\smallbreak\noindent\textbf{Log truncation.}
The algorithm as specified in Figure~\ref{fig:code} retains operations in the log indefinitely, so the memory use grows without bound.
However, in practice it is easy to truncate the log, because \isa{apply\_op} only examines the log prefix of operations whose timestamp is greater than that of the operation being applied.
Thus, once it is known that all future operations will have a timestamp greater than $t$, then operations with timestamp $t$ or less can be discarded from the log.
In this case, we say that $t$ is \emph{causally stable}~\cite{Baquero:2014ed}.

A similar approach can be used to garbage-collect any tree nodes in the trash (\S\ref{sec:ops-trees}).
Initially, trashed nodes must be retained because a concurrent move operation may move them back out of the trash.
However, once the operation that moved a node to the trash is causally stable, we know that no future operations will refer to this node, and so the trashed node and its descendants can be discarded.

We can determine causal stability in a system where the set of replicas is known, where each replica generates operations with monotonically increasing timestamps, and where the communication link between any pair of replicas is FIFO (messages are received in the order in which they are sent, as implemented e.g.\ by TCP).
In this case, we can keep track of the most recent timestamp we have seen from each replica (including our own), and the minimum of these timestamps is the causally stable threshold.

\smallbreak\noindent\textbf{Ordering of sibling nodes.}
Another useful extension of the tree algorithm is to allow children of the same parent node to have an explicit ordering.
For example, in XML, the set of children of an element is ordered.
This can be implemented by maintaining an additional list CRDT for each branch node, e.g.\ using RGA~\cite{Roh:2011dw} or Logoot~\cite{Weiss:2010hx}.
These algorithms assign a unique ID to each element of the list, and this ID can be included in the metadata field of move operations in order to determine the order of sibling nodes.

This approach to determining ordering also easily supports reordering of child nodes within a parent: to move a node to a different position in a list, we use the list CRDT to generate a new ID at the desired position in the sequence~\cite{Kleppmann:2020jw}.
Then we perform a move operation in which the parent node is unchanged, and this new ID is used as metadata.

%\smallbreak\noindent\textbf{File merging.}
%In a distributed filesystem, replication and conflict resolution is required not only for the directory structure, but also for the contents of individual files.
%This can be accomplished by using CRDTs for file contents as well.
%We discuss distributed filesystems further in \S\ref{sec:filesystems}.

\section{Proof of correctness}\label{sec:proof}

We now discuss the correctness properties of the algorithm from \S\ref{sec:algorithm}.
All theorems stated here have been formally proved and mechanically checked using Isabelle.
For space reasons this paper gives only the statements that were proved, but elides a discussion of the reasoning steps.
The Isabelle files containing the full details are open source~\cite{SourceFiles}, and are included as supplementary material with this paper.

To reason about the state of a replica we first define the function \isa{apply\_ops} on lines 51--52 of Figure~\ref{fig:code}.
It takes a list of operations \isa{ops} and returns the state of a replica after it has applied all the operations in \isa{ops}.
The \isa{apply\_ops} function works by starting in the initial state \isa{([], \{\})} consisting of the empty operation log \isa{[]} and the tree represented by the empty set \isa{\{\}}, and then applying the operations one by one to the state using the \isa{apply\_op} function (introduced in \S\ref{sec:applying}).
The \isa{foldl} function from the Isabelle/HOL standard library performs the iteration over the list of operations.

\subsection{Tree invariants}\label{sec:tree-invariants}

A tree is an acyclic graph in which every node has exactly one parent, except for the root, which has no parent.
In fact, we slightly generalise this property and allow more than one root to exist, so the graph represents a forest, allowing an application to move nodes between different trees if desired.
For example, the trash node used for deletion can be separate from the main tree.
To prove that our algorithm maintains a forest structure, no matter which operations are applied, we demonstrate several invariants.

\smallbreak\noindent\textbf{Each node's parent is unique.}
The first invariant we prove is that each tree node has either no parent (if it is the root of a tree) or exactly one parent (if it is a non-root node).
We state this theorem in Isabelle/HOL as follows, where \isa{apply\_ops\_unique\_parent} is the name of this theorem:
\begin{isabelle}
\isacommand{theorem}\isamarkupfalse%
\ apply{\isacharunderscore}ops{\isacharunderscore}unique{\isacharunderscore}parent{\isacharcolon}\isanewline
\ \ \isakeyword{assumes}\ {\isacartoucheopen}apply{\isacharunderscore}ops\ ops\ {\isacharequal}\ {\isacharparenleft}log{\isacharcomma}\ tree{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \isakeyword{shows}\ {\isacartoucheopen}unique{\isacharunderscore}parent\ tree{\isacartoucheclose}
\end{isabelle}
\noindent That is, we consider any list of operations \isa{ops} and define \isa{(log, tree)} to be the replica state after \isa{ops} have been applied.
We then prove that ``\isa{unique\_parent tree}'' holds, where the \isa{unique\_parent} predicate is defined on lines 54--56 of Figure~\ref{fig:code}: whenever the tree contains a triple whose third element is the child node \isa{c}, then the first and second elements of the triple (the parent node and the metadata) are uniquely defined.
As we make no assumptions about \isa{ops}, this theorem holds for any replica state that can be reached by applying any number of operations.

\smallbreak\noindent\textbf{The graph contains no cycles.}
This second invariant is expressed as follows in Isabelle/HOL:
\begin{isabelle}
\isacommand{theorem}\isamarkupfalse%
\ apply{\isacharunderscore}ops{\isacharunderscore}acyclic{\isacharcolon}\isanewline
\ \ \isakeyword{assumes}\ {\isacartoucheopen}apply{\isacharunderscore}ops\ ops\ {\isacharequal}\ {\isacharparenleft}log{\isacharcomma}\ tree{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \isakeyword{shows}\ {\isacartoucheopen}acyclic\ tree{\isacartoucheclose}
\end{isabelle}
\noindent The \isa{acyclic} predicate is defined on lines 58--59 of Figure~\ref{fig:code}, using the \isa{ancestor} relation (the transitive closure of the graph's edges): a graph contains no cycles if no node is an ancestor of itself.

\smallbreak\noindent\textbf{Other correctness properties.}
There are further criteria we might use to determine if our algorithm is correct.
For example, we might demonstrate that a single-replica system operates with the usual sequential semantics of a tree.
In a system with multiple disjoint trees, we could prove that the trees don't get tangled together.
We conjecture that those properties hold for our algorithm, but leave the proof out of scope for this paper.

\subsection{Convergence}\label{sec:convergence}

As discussed in \S\ref{sec:applying}, we require that when replicas apply the same set of operations, they converge towards the same state, regardless of the order in which the operations are applied.
We formalise this in Isabelle/HOL as follows:
\begin{isabelle}
\isacommand{theorem}\isamarkupfalse%
\ apply{\isacharunderscore}ops{\isacharunderscore}commutes{\isacharcolon}\isanewline
\ \ \isakeyword{assumes}\ {\isacartoucheopen}set\ ops{\isadigit{1}}\ {\isacharequal}\ set\ ops{\isadigit{2}}{\isacartoucheclose}\isanewline
\ \ \ \ \isakeyword{and}\ {\isacartoucheopen}distinct\ {\isacharparenleft}map\ move{\isacharunderscore}time\ ops{\isadigit{1}}{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \ \ \isakeyword{and}\ {\isacartoucheopen}distinct\ {\isacharparenleft}map\ move{\isacharunderscore}time\ ops{\isadigit{2}}{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \isakeyword{shows}\ {\isacartoucheopen}apply{\isacharunderscore}ops\ ops{\isadigit{1}}\ {\isacharequal}\ apply{\isacharunderscore}ops\ ops{\isadigit{2}}{\isacartoucheclose}
\end{isabelle}

The predicate \isa{distinct} takes a list as argument, and returns true if all elements of the list are distinct (i.e.\ no value occurs more than once in the list).
Therefore, the assumption \isa{distinct (map move\_time ops1)} states that in the list \isa{ops1}, there are no two operations with the same timestamp.

The function \isa{set} takes a list and turns it into an unordered set with the same elements.
Thus, the assumption \isa{set ops1 = set ops2} means that the lists \isa{ops1} and \isa{ops2} contain the same elements, but perhaps in a different order---in other words, \isa{ops1} is a permutation of \isa{ops2}.
Under these assumptions, \isa{apply\_ops\_commutes} proves that applying the list of operations \isa{ops1} results in the same replica state as applying the list of operations \isa{ops2}.

\smallbreak\noindent\textbf{Strong eventual consistency.}
Gomes et al.~\cite{Gomes:2017gy} define a framework in Isabelle/HOL for proving the strong eventual consistency properties of CRDTs.
Using our convergence proof above we integrate our tree datatype with this framework, and thus demonstrate that our move operation on trees does indeed guarantee strong eventual consistency.
The details appear in our Isabelle theory files~\cite{SourceFiles}.

\subsection{Making the HOL definitions executable} \label{subsect.extracting}

Isabelle/HOL can generate executable Haskell, OCaml, Scala, and Standard ML code from HOL definitions using a sophisticated code generation mechanism~\cite{DBLP:conf/flops/HaftmannN10}.
However, not all definitions can be realised in executable form: for example, the use of choice principles (like in \isa{get\_parent}) and inductively defined relations (e.g.\ \isa{ancestor}) cause problems.
Moreover, HOL's \isa{set} type allows infinite sets.
Whilst these constructs are convenient for theorem proving, they do not translate well to executable code.

We therefore produce variants of the definitions in Figure~\ref{fig:code} that are designed for execution rather than theorem proving.
Rather than representing the tree as a set of (parent, meta, child) triples, these definitions use a hash-map in which the keys are child nodes, and the values are (meta, parent) pairs, written in Isabelle/HOL as \isa{('n::\{hashable\}, 'm {\isasymtimes} 'n) hm}.
The \isa{hashable} type class means that keys must have a hashing function.
In effect, this hash-map is an index over the set of triples, using the fact that the parent and metadata for a given child are unique (\S\ref{sec:tree-invariants}).
The hash-map implementation is from the Isabelle Collections Framework~\cite{DBLP:conf/itp/LammichL10}.

A hash-map \isa{t} of type \isa{('n, 'm {\isasymtimes} 'n) hm} \emph{simulates} a set \isa{T} of type \isa{('n {\isasymtimes} 'm {\isasymtimes} 'n) set} when their entries are the same:
\begin{isabelle}
\isacommand{definition}\isamarkupfalse%
\ simulates\ \isakeyword{where}\ {\isacartoucheopen}simulates\ t\ T\ {\isasymequiv}\isanewline
\ \ {\isacharparenleft}{\isasymforall}p\ m\ c{\isachardot}\ hm{\isachardot}lookup\ c\ t\ {\isacharequal}\ Some\ {\isacharparenleft}m{\isacharcomma}\ p{\isacharparenright}\ {\isasymlongleftrightarrow}\ {\isacharparenleft}p{\isacharcomma}\ m{\isacharcomma}\ c{\isacharparenright}\ {\isasymin}\ T{\isacharparenright}{\isacartoucheclose}
\end{isabelle}
\noindent where \isa{hm.lookup c t} looks up the key \isa{c} in the hash-map \isa{t}, returning \isa{Some x} if \isa{c} maps to the value \isa{x}, and returning \isa{None} if \isa{c} does not appear in the hash-map.
We can now prove the equivalence of the set-based and the hash-map-based implementations:
\begin{isabelle}
\isacommand{lemma}\isamarkupfalse%
\ executable{\isacharunderscore}apply{\isacharunderscore}ops{\isacharunderscore}simulates{\isacharcolon}\isanewline
\ \ \isakeyword{assumes}\ {\isacartoucheopen}executable{\isacharunderscore}apply{\isacharunderscore}ops\ ops\ {\isacharequal}\ {\isacharparenleft}log{\isadigit{1}}{\isacharcomma}\ t{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \ \ \isakeyword{and}\ {\isacartoucheopen}apply{\isacharunderscore}ops\ ops\ {\isacharequal}\ {\isacharparenleft}log{\isadigit{2}}{\isacharcomma}\ T{\isacharparenright}{\isacartoucheclose}\isanewline
\ \ \isakeyword{shows}\ {\isacartoucheopen}log{\isadigit{1}}\ {\isacharequal}\ log{\isadigit{2}}\ {\isasymand}\ simulates\ t\ T{\isacartoucheclose}
\end{isabelle}

That is, if \isa{executable\_apply\_ops} and \isa{apply\_ops} are applied to the same list of operations, they produce identical logs, and also produce trees that contain the same set of key-value bindings---i.e. the trees are extensionally equivalent, despite having very different in-memory representations.
We can also prove corollaries of \isa{apply\_ops\_commutes} and \isa{apply\_ops\_acyclic} for the hash-map-based implementation.
%The analogue of \isa{apply\_ops\_unique\_parent} holds trivially since by construction, the hash-map contains at most one (metadata, parent) pair for a given child.

\begin{figure}
  \begin{center}
  \includegraphics{crdt-generated.pdf}
  \end{center}
  \caption{Median execution time to apply an operation to the replica state, using Scala code generated by Isabelle. Error bars indicate the minimum and 95th percentile.}
  \label{fig:plot-generated}
\end{figure}

\begin{figure}
  \begin{center}
  \includegraphics{crdt-optimised.pdf}
  \end{center}
  \caption{Median execution time to apply an operation to the replica state, using a hand-optimised (not formally verified) Scala implementation. Error bars indicate the minimum and 95th percentile.}
  \label{fig:plot-optimised}
\end{figure}

\section{Evaluation}\label{sec:evaluation}

\subsection{Performance of move operation}

With our algorithm, the worst-case cost of applying a move operation is $O(nd)$, where $n$ is the number of operations in the log that need to be undone and redone, and $d$ is the depth of the tree (the number of parent relationships that need to be traversed to check whether the operation would introduce a cycle).
To demonstrate that this cost is acceptable in practice, we evaluated the performance of two implementations of our algorithm.
The first implementation is Scala code extracted from our formally verified HOL definitions using Isabelle/HOL's code generation mechanism.
The second implementation is handwritten Scala code that we believe to be functionally equivalent, but which we have not formally verified.
By comparing these implementations we can distinguish between inefficiencies introduced by code generation and costs that are inherent to our algorithm.

We wrapped both implementations in a simple network service and deployed three replicas on Amazon EC2 \texttt{c5.large} instances in Northern California (\texttt{us-west-1}), Ireland (\texttt{eu-west-1}), and Singapore (\texttt{ap-southeast-1}).
The network delays between these regions are substantial: a round trip from Ireland to California takes a median of 145~ms, and from Singapore to California takes 176~ms.

\begin{figure*}
  \begin{center}
  \includegraphics{leader-vs-crdt.pdf}
  \end{center}
  \caption{Median time to apply a local CRDT operation compared to the median time to perform a move operation using state machine replication, with a leader located in another region. Note the log scale on both axes.}
  \label{fig:leader-plot}
\end{figure*}

We use a synthetic workload in which each replica starts with an empty tree and generates move operations at a fixed rate; for each move operation the generating replica chooses parent and child nodes uniformly at random from a set of 1,000 tree nodes.
A tree node is created by the first operation that refers to it, and for simplicity we do not use the optimisation discussed in \S~\ref{sec:create}.
We use Lamport clocks~\cite{Lamport:1978jq} as operation timestamps, and 64-bit integers to identify tree nodes.
When a replica generates an operation, it immediately applies that operation to its local state, and it asynchronously sends the operation to the other two replicas via TCP connections.
When a replica receives an operation from a remote replica, it also applies that operation to its own replica state.
Thus, of the operations applied by each replica, approximately one third are locally generated, and two thirds are received from the other two replicas.
A replica does not wait for a response for the previous operation before generating and sending the next, so there may be many operations ``in flight'' at the same time.

For a given operation rate we ran the system for 10 minutes to reach a steady state.
We repeated the experiment with different operation rates, and measured the execution time of our algorithm for applying each operation.
Figures~\ref{fig:plot-generated} and~\ref{fig:plot-optimised} show the results for the Isabelle-generated code and the handwritten code respectively.
In each figure, the upper plot shows the distribution of execution times to apply one operation received from another replica, and the lower plot shows the time to apply one locally generated operation.

Both implementations exhibit qualitatively similar behaviour, but the absolute numbers differ significantly.
A local operation takes near-constant time to apply because its timestamp is always greater than any existing operation at the generating replica (by definition of Lamport clocks), so it does not require any undo or redo.
The median time to apply a local operation is around 50~{\textmu}s for the Isabelle-generated code and around 1--2~{\textmu}s for the handwritten code.
The reduced performance at low operation rates can be explained by the JVM delaying JIT optimisations until a method has been executed a certain number of times.

For remote operations, the execution time increases proportionally to the rate at which operations are generated: as the time interval between successive operations decreases relative to the network delay, more operations are in flight at the same time, and more undos/redos are required to put operations in timestamp order.
At some point, each single-threaded replica is fully utilising one CPU core, and increasing the rate at which operations are generated does not increase throughput any further.
The Isabelle-generated code is saturated at a rate of 600 operations/sec (median remote operations taking $\approx$1~ms), while the optimised code is saturated at 5,700 operations/sec.
This factor of 9.5 performance difference is solely due to inefficient code generation; functionally both implementations perform the same work.

At peak, the optimised implementation is performing on average 200 undos and redos per remote operation.
The throughput could be increased by applying a batch of operations at once, which would allow the cost of the undos and redos to be amortised over the size of the batch.
We leave an evaluation of this optimisation for future work.

% Note that when a user is interacting with the system, they only have to wait for local operations to execute; any remote operations can be applied in the background without affecting user interaction.
% Since our algorithm need not perform any undos or redos for local operations, these user interactions are consistently fast.

\subsection{Comparison to locking/state machine replication}\label{sec:smr}

An alternative to our CRDT algorithm is to use a locking protocol (\S~\ref{sec:impossibility}).
For example, a replica can acquire a lock from a lock server, perform an update, and then release the lock again.
The minimum time for which a lock will be held after it is acquired is then the round-trip time.
If the lock server is in California and the replica is in Singapore, and if one move operation is performed per lock acquisition, this system could only perform $1/176\,\mathrm{ms} = 5.7$ operations/sec.
The throughput of this locking-based scheme is therefore three orders of magnitude lower than our optimised algorithm's throughput of 5,700 operations/sec.

A better alternative to our CRDT algorithm is to use state machine replication~\cite{Schneider:1990vy}: that is, we use a leader replica or consensus algorithm to impose a total order on all operations, and then execute operations in that same order on all replicas.
For a move operation on trees, the state machine replication algorithm is much simpler than the CRDT: it still needs to check for cycles, but it never needs to undo or redo any operations because they are never applied out-of-order.

To compare the performance of our algorithm to the state machine approach we ran another set of experiments on the same three replicas in California, Ireland, and Singapore.
In this experiment, the Californian replica was designated leader; it totally ordered all operations it received, and sent them to all other replicas in the same order.
The Irish and Singaporean replicas generated operations, sent them to the leader, and applied them to their local tree in the order they were received from the leader.
In order to ensure a fair comparison to the CRDT algorithm, we ran these experiments using the same two implementations (Isabelle-generated and handwritten) and the same networking code.

The results are shown in Figure~\ref{fig:leader-plot}.
Using the Isabelle-generated code, the leader-based approach is able to sustain 14,000 move operations per second (23 times the CRDT's throughput of 600 ops/sec).
Using the handwritten code, the leader-based throughput is 22,000 ops/sec (4 times the CRDT's throughput of 5,700 ops/sec).
The downside of state machine replication is that performing an operation requires waiting for a round-trip to the leader, which implies around five orders of magnitude higher latency (145--176~ms) than the 1--2~{\textmu}s it takes to execute local CRDT operations.

Therefore, we have a clear trade-off: in applications that need to maximise throughput, a state machine replication approach is preferable; in applications that need to minimise response times to user requests or that need to continue to be available during network interruptions, our CRDT algorithm is preferable.
In the leader-based approach, clients cannot make updates while offline.

\subsection{Evaluation of formal proof}

The formalisation of our algorithm, and the proofs of its properties as described in \S\ref{sec:proof}, have been formally checked by Isabelle/HOL.
Our proofs contain no unproven assumptions (i.e. no occurrences of the \textbf{sorry} keyword).
Checking all of the proofs takes 3 minutes on a 2018 MacBook Pro.

Besides the 59 lines of definitions given in Figure~\ref{fig:code}, our Isabelle/HOL formalisation consists of a further 2,495 lines of proof code.
Of this, we use 203 lines to prove that every node has a unique parent, 443 lines to prove that the tree contains no cycles, 450 lines to prove that move operations commute and replicas converge, 327 lines to prove the strong eventual consistency property, 743 lines to define the executable variant of our algorithm and prove its equivalence to the definitions of Figure~\ref{fig:code}, and 779 lines to prove the safety of the optimisation in \S\ref{sec:create}.

\section{Related work}\label{sec:relwork}

Many replicated data systems use optimistic replication~\cite{Saito:2005jw}, which allows the state of replicas to temporarily diverge, in order to achieve better performance and availability in the presence of faults than strongly consistent systems~\cite{Bailis:2014th,Gilbert:2002il}.
As a consequence, these systems require a mechanism for merging or reconciling conflicting updates that were made concurrently on different replicas.
For example, version control systems such as Git~\cite{Chacon:2014kr} leave conflicts for the user to resolve manually.
Databases such as Dynamo~\cite{DeCandia:2007ui} and Bayou~\cite{Terry:1995dn} rely on the application programmer to provide explicit conflict resolution logic; however, such logic is difficult to get right \cite{Bailis:2013jc,Burckhardt:2014hy,Gomes:2017gy}.
Hence, we want to automatically ensure that all replicas converge towards a consistent state, without requiring custom application logic---a consistency model known as \emph{strong eventual consistency}~\cite{Shapiro:2011un,Gomes:2017gy}.

\subsection{Conflict-free Replicated Data Types}

Our algorithm is an example of an operation-based Conflict-free Replicated Data Type or CRDT~\cite{Shapiro:2011un,Burckhardt:2014ft}.
All CRDTs share the property that concurrent changes on different replicas can be merged in any order; any two replicas that have seen the same set of updates are guaranteed to be in the same state, regardless of the order in which they processed these updates.
Several CRDTs for trees have been proposed:
\begin{itemize}
    \item Martin et al.~\cite{Martin:2010ih,Martin:2011} define a CRDT for XML data, and Kleppmann and Beresford~\cite{Kleppmann:2016ve} define a CRDT for JSON.
        However, these algorithms only deal with insertion and deletion of tree nodes, and do not support moves.
        A move operation can be emulated by deleting and re-inserting the moved node, but this approach suffers from the duplication problem demonstrated in \S\ref{sec:move-same-item}.
    \item As discussed in \S\ref{sec:impossibility}, Najafzadeh et al.~\cite{Najafzadeh:2017vk,Najafzadeh:2018bw} propose two implementations for a replicated filesystem: a CRDT in which conflicting moves are handled by duplicating tree nodes (as in Figures~\ref{fig:move-same-item}b and~\ref{fig:move-cycle}b), and a centralised implementation in which move operations must obtain a lock before executing (not a CRDT since it relies on synchronous coordination).
        We discuss the performance costs of locking in \S\ref{sec:smr}.
    \item Ahmed-Nacer et al.~\cite{AhmedNacer:2012us} outline approaches to handling conflicts on trees, but provide no algorithms.
    \item Tao et al.~\cite{Tao:2015gd} propose handling conflicting move operations by allowing the same object to appear in more than one location; thus, their datatype is strictly a DAG, not a tree.
        Some conflicts are handled by duplicating tree nodes.
        Tao et al.\ also perform experiments with Dropbox, Google Drive, and OneDrive, similar to our experiments discussed in \S\ref{sec:move-is-hard}.
    \item Nair et al.~\cite{Nair:2021} develop a CRDT tree with move operation.
        This work is concurrent to ours and it is unpublished at the time of writing, so we have not been able to conduct a detailed comparison.
        % TODO now available on arxiv: https://arxiv.org/abs/2103.04828
\end{itemize}

Several other CRDTs, such as Treedoc~\cite{Preguica:2009fz} and LSEQ~\cite{Nedelec:2013ky}, use a tree structure internally.
However, their data model is a linear sequence; the tree structure is not accessible to the application (for example, an application cannot freely choose the parent node of a new tree node), and they do not provide a move operation.

Besides CRDTs, another family of algorithms for concurrent modification of data structures is \emph{Operational Transformation} (OT)~\cite{Sun:1998vf}.
Several authors have defined concurrent tree structures using OT \cite{Jungnickel:2016cb,Ignat:2003jy,Davis:2002iv}, but they only handle insertion and deletion of nodes, and do not support moves.

Molli et al.~\cite{Molli:2003cd} define an OT tree structure with a move operation.
However, it requires that all communication between replicas is performed via total order broadcast, which requires a leader replica or consensus algorithm, like in \S\ref{sec:smr}.
Our algorithm has better availability characteristics in the presence of network partitions because it allows messages to be delivered in any order, e.g.\ via peer-to-peer protocols.

Collaborative graphics software Figma uses an approach inspired by CRDTs, but prevents cycles in their object tree by relying on a central server; its replication protocol allows objects to temporarily disappear while syncing~\cite{Wallace:2019vf}.

\subsection{Distributed filesystems}\label{sec:filesystems}

Many distributed filesystems, such as NFS, rely on synchronous interaction with a server.
This avoids the need for conflict resolution, but rules out users working offline.

Coda is a client-server filesystem that allows clients to locally cache copies of files stored in a server-side data repository~\cite{kistler1992coda}. 
Clients can edit data in the cache while offline, during which time a kernel module keeps track of all updates. 
When the client comes back online it attempts to resynchronise changes with the server. 
To resolve conflicts due to concurrent updates, Coda uses application-specific resolvers~\cite{Kumar:1995wf}, similarly to Bayou's approach~\cite{Terry:1995dn}.
Concurrent renaming and move operations have been considered, but the authors note that they do ``not address transparent resolution of cross-directory renames [i.e.\ move operations] in [their] current implementation''~\cite{kumar1993log}.
Furthermore, while the authors consider a number of conflicts associated with directory move operations, they do not highlight the potential for the creation of cycles.

Ficus~\cite{Reiher:1994wh} is an in-kernel SunOS-based replicated peer-to-peer filesystem. 
Ficus supports updates to replicas during periods of network partition and claims ``conflicting updates to directories are detected and automatically repaired''~\cite{guy1990implementation}. 
Unfortunately we were unable to find a precise definition of the algorithm used in any of the available publications. 

Rumor~\cite{Guy:1999gy,RumorManual} is the successor to Ficus.
While previous work uses the kernel filesystem interface, Rumor is a userspace process that is invoked periodically by the user or by a daemon; when run, it compares the state of the replicas.
The original version of Rumor was unable to scale beyond 20 replicas, but an extension called Roam~\cite{Ratner:1999fh} allowed better scaling. 
In an attempt to test Rumor's conflict handling we obtained the source code from \emph{archive.org}~\cite{RumorSource}; however, we were unable to get it running after modest effort.
%The user manual \cite{RumorManual} makes the claim that "Unix directories are an important example of a kind of file for which Rumor automatically resolves conflicts"

Unison is a file synchronisation tool with a formal specification that allows two replicas to synchronise the state of a directory~\cite{PierceVouillon:UnisonSpecTR}.
It permits offline updates to both replicas.
Like Rumor, Unison is a userspace process that compares replica states.
Whenever it is run, Unison records a summary of the filesystem state on each replica, and it uses this summary to determine the changes made since the last synchronisation. 
When presented with the move operations described in Figure~\ref{fig:move-same-item}, Unison duplicates the files, resulting in the outcome shown in Figure~\ref{fig:move-same-item}a. 
Unison is unable to automatically synchronise the move operations shown in Figure~\ref{fig:move-cycle} and instead asks the user to choose one of four possible resolutions: those shown in Figure~\ref{fig:move-cycle}b, \ref{fig:move-cycle}c, \ref{fig:move-cycle}d, or to delete both directories.

Hughes et al.~\cite{Hughes:2016fp} test Dropbox and Google Drive against a formal specification, but they do not consider moving files, and thus they do not find the issue described in \S\ref{sec:move-cycle}.

Bj{\o}rner~\cite{Bjorner:2007hp} discusses the development of the Distributed File System Replication (DFS-R) component of Windows Server, during which a model checker found an issue with concurrent moves similar to Figure~\ref{fig:move-cycle}a.
Bj{\o}rner outlines several possible solutions, but notes that model-checking their algorithm was not feasible due to state space explosion.
Our use of proof by induction, rather than model-checking, allows us to verify the correctness of our algorithm in unbounded executions.

After we performed the experiments described in \S\ref{sec:move-is-hard}, the Dropbox engineering team published a blog post~\cite{Dropbox2020sync} acknowledging the problems of cycles and duplication due to concurrent moves.

\subsection{Totally ordered operation log}\label{sec:oplogs}

Our approach of ordering operations by a timestamp, and undoing/redoing them as necessary so that they take effect in ascending timestamp order, is conceptually very simple.
Similar ideas appear in many other systems, including the Bayou database~\cite{Terry:1995dn}, Jefferson's \emph{Time Warp} mechanism~\cite{Jefferson:1985em}, and Burckhardt's \emph{standard conflict resolution}~\cite[{\S}4.3.3]{Burckhardt:2014hy}.
The concept of undo-redo is also well known from write-ahead logging~\cite{Mohan:1992fe}.

The SECRO approach~\cite{DePorre:2019} allows arbitrary CRDTs to be defined by having each replica execute deterministic update functions in the same total order, and replaying operation history if necessary.
In principle, our algorithm could be expressed as a SECRO, but we provide optimisations that go beyond the SECRO model.
SECRO always processes operations in forward direction, whereas our use of undo/redo is more efficient on long operation histories if replicas have most of the operation log in common.
(SECRO allows long operation histories to be truncated, but this mechanism may result in discarded operations if replicas are disconnected for extended periods of time.)
Moreover, our optimisation of node creation operations (\S\ref{sec:create}) is specific to our algorithm, and is not possible in the general SECRO model.

As an example of the SECRO approach, De Porre et al.\ present an AVL tree CRDT~\cite{DePorre:2019}.
Even though this data structure is internally a tree, the interface it exposes is an ordered set datatype, not a tree in which the user can choose to create, delete, and move arbitrary nodes.

To our knowledge, the approach of ordering operations by timestamp has not previously been applied to the problem of replicated trees, and in particular not a move operation.
We are not aware of previous mechanised proofs (using Isabelle or other tools) that formalise this approach.

\section{Conclusions}

We have defined a novel algorithm that handles arbitrary concurrent modifications of a tree data structure---adding, moving, and removing nodes---in a peer-to-peer replication setting.
It is applicable to distributed filesystems, databases, and applications that use a tree-structured data model.
Our approach ensures that all replicas converge to the same consistent state without needing any manual conflict resolution, and without requiring application developers to implement conflict handling logic.
Updates made to a local replica take effect immediately, while operations from remote replicas can be propagated and applied asynchronously.
This approach means that user interaction is consistently fast, even in the face of unbounded communication delays or during disconnected operation of mobile devices.

The principle behind our algorithm is easy to understand: undoing and redoing operations so that they are effectively executed in timestamp order.
Nevertheless, it solves a real problem that has not been solved correctly in widely deployed software such as Google Drive and Dropbox, as we demonstrated in \S\ref{sec:move-is-hard}.
To rule out such bugs, we formally verified the correctness of our algorithm using the Isabelle/HOL proof assistant; our theorems show that replicas can apply operations in any order, and that the result is always a valid tree (nodes have at most one parent, and the graph does not contain any cycles).
Moreover, these results apply to unbounded executions and an arbitrary number of replicas---an advantage of using a proof assistant over other formal approaches such as model checking.

One might wonder whether our algorithm's consistency model is strong enough for practical use.
On this point, we note that this model is what Google Drive and Dropbox use today~\cite{Hughes:2016fp} (apart from the aforementioned bugs).

We also evaluated the performance of two implementations of our algorithm (one formally verified, the other optimised for performance) across three replicas in California, Ireland and Singapore.
Our optimised implementation applies local updates in 1--2~{\textmu}s, five orders of magnitude faster than is possible with a leader replica or consensus protocol operating over these distances, and our algorithm remains available in the face of network interruptions.
However, compared to leader-based replication our algorithm has four times lower throughput.

Our work is especially interesting due to the ubiquity of tree data models across many different types of applications and databases.
In future work we hope to integrate our algorithm into CRDT libraries such as Automerge,\footnote{\url{https://github.com/automerge/automerge}} and use it to build novel collaborative applications.

We are also exploring whether the undo-do-redo approach can be used in other concurrent data structures; for example, there is an open problem in collaborative text editing~\cite{Kleppmann:2020jw} that might be solved by this approach.
Our Isabelle/HOL formalisation, which is open source~\cite{SourceFiles}, can be used for future work in this area.

\section*{Acknowledgements}

The authors wish to acknowledge the support of The Boeing Company and the EPSRC ``REMS: Rigorous Engineering for Mainstream Systems'' programme grant (EP/K008528).
Martin Kleppmann is supported by a Leverhulme Trust Early Career Fellowship, the Isaac Newton Trust, Nokia Bell Labs, and crowdfunding supporters including Ably, Adri Arcarons, Chet Corcos, Macrometa, Mintter, David Pollak, RelationalAI, SoftwareMill, Talent Formation Network, and Adam Wiggins.
Our evaluation was conducted using AWS credits from the AWS Educate program.
Thank you to Marc Shapiro for feedback on a draft of this paper.

\bibliographystyle{IEEEtran}
\bibliography{references}{}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo-martin.jpg}}]{Martin Kleppmann}
is a Senior Research Associate at the University of Cambridge. His research spans distributed systems, databases, security, and formal verification, with a focus on decentralised collaboration software and CRDTs. His book \emph{Designing Data-Intensive Applications} was published in 2017 and has been translated into six languages. Previously, he was a software engineer and entrepreneur at Internet companies including Rapportive and LinkedIn.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo-dominic.jpg}}]{Dominic P.\ Mulligan}
is a Principal Research Engineer in the Systems Group at Arm Research, UK.  Prior to moving to Arm, he worked as a postdoctoral researcher at the Universities of Cambridge and Bologna, Italy, investigating the formal specification and verification of systems software such as C compilers and linkers.  His interests include formal verification, distributed systems, and privacy and security.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo-victor.jpg}}]{Victor B.\ F.\ Gomes}
received a diplme d'ingnieur from INSA Lyon and a PhD degree from the University of Sheffield. He was a research associate at the University of Cambridge from 2016 to 2019 and he is currently working at Google. His main research interests are semantics of programming languages, formal verification via theorem prover assistants and algebraic approaches for program verification.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{photo-alastair.jpg}}]{Alastair R.\ Beresford}
is Professor of Computer Security in the Department of Computer Science at the University of Cambridge. His research work explores the security and privacy of large-scale distributed systems, with a particular focus on networked mobile devices such as smartphones, tablets and laptops. He looks at the security and privacy of the devices themselves, as well as the security and privacy problems induced by the interaction between mobile devices and cloud-based Internet services.
\end{IEEEbiography}
\vfill
\end{document}
